"""
LLM Policy Router Pydantic Schemas

This module defines the Pydantic schemas for LLM Policy Router data validation
and API contracts.

Key schemas:
- LLMScope: Enum-like scope identifiers
- LLMTier: Agent tier identifiers
- RoutingMode: Routing behavior modes
- ResolvedLLMCall: The core "call contract" returned by the router
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, HttpUrl, SecretStr, field_validator


class LLMScope(str, Enum):
    """
    LLM scope identifiers.

    Scopes represent business intents and determine routing behavior:
    - SYSTEM_ONLY scopes: Always use system profile (never BYOK)
    - AUTO scopes: Use BYOK if eligible and configured, else system profile
    """

    # SYSTEM_ONLY scopes
    COPILOT = "copilot"
    COCKPIT_SCAN = "cockpit_scan"
    CREW_ROUTER = "crew_router"
    CREW_SUMMARY = "crew_summary"
    QUICK_SCAN = "quick_scan"
    CHART_SCAN = "chart_scan"

    # AUTO scopes (tiered agents)
    AGENTS_FAST = "agents_fast"
    AGENTS_BALANCED = "agents_balanced"
    AGENTS_BEST = "agents_best"

    @classmethod
    def is_system_only(cls, scope: str) -> bool:
        """Check if scope requires system-only routing."""
        return scope in [
            cls.COPILOT.value,
            cls.COCKPIT_SCAN.value,
            cls.CREW_ROUTER.value,
            cls.CREW_SUMMARY.value,
            cls.QUICK_SCAN.value,
            cls.CHART_SCAN.value,
        ]

    @classmethod
    def is_agent_tier(cls, scope: str) -> bool:
        """Check if scope is an agent tier (supports BYOK)."""
        return scope in [
            cls.AGENTS_FAST.value,
            cls.AGENTS_BALANCED.value,
            cls.AGENTS_BEST.value,
        ]


class LLMTier(str, Enum):
    """
    Agent tier identifiers for BYOK profiles.

    These map to internal scopes:
    - FAST → agents_fast
    - BALANCED → agents_balanced
    - BEST → agents_best

    Product UX names (Eco/Standard/Extreme) should be mapped to these
    at the Entitlement/UX layer, NOT in the LLM core.
    """

    FAST = "agents_fast"
    BALANCED = "agents_balanced"
    BEST = "agents_best"


class RoutingMode(str, Enum):
    """
    Routing mode for overrides.

    - SYSTEM_ONLY: Force system profile (ignore BYOK)
    - USER_BYOK_ONLY: Require BYOK profile (error if missing)
    - AUTO: Default routing logic (use BYOK if available, else system)
    """

    SYSTEM_ONLY = "SYSTEM_ONLY"
    USER_BYOK_ONLY = "USER_BYOK_ONLY"
    AUTO = "AUTO"


class VirtualKeyType(str, Enum):
    """Virtual key types."""

    USER = "user"  # For BYOK agent tiers
    SYSTEM_ON_BEHALF = "system_on_behalf"  # For system scopes


class VirtualKeyStatus(str, Enum):
    """Virtual key status."""

    ACTIVE = "active"
    PROVISIONING = "provisioning"
    FAILED = "failed"
    REVOKED = "revoked"


# ============================================================================
# Core Call Contract
# ============================================================================


class ResolvedLLMCall(BaseModel):
    """
    The resolved LLM call contract returned by LLMPolicyRouter.

    This is the ONLY data structure that the router returns, and it contains
    everything needed to make a call to the LiteLLM Proxy:

    - base_url: LiteLLM proxy endpoint
    - api_key: Virtual key (vk_user or vk_system_on_behalf)
    - model: Model alias (sys_* or agents_*)
    - metadata: Tags for logging/audit (includes run_id)
    - extra_headers: Governance headers (message redaction)
    - extra_body: Optional user_config for BYOK

    Security:
    - api_key is SecretStr (never logged in plaintext)
    - user_config in extra_body contains decrypted BYOK keys (ephemeral)
    - Tags in metadata enable tracing in LiteLLM Dashboard

    IMPORTANT: run_id and tags MUST be generated by Router, NOT by business code.
    This ensures consistent tracing and prevents tag fragmentation.
    """

    # Required fields
    base_url: HttpUrl = Field(
        ...,
        description="LiteLLM Proxy base URL (e.g., http://litellm:4000/v1)",
    )
    api_key: SecretStr = Field(
        ...,
        description="LiteLLM Virtual Key (vk_user or vk_system_on_behalf)",
    )
    model: str = Field(
        ...,
        description="Model alias (sys_copilot_v1, agents_fast, etc.)",
        min_length=1,
    )

    # Metadata (放请求体，不用自定义 header)
    metadata: Dict[str, Any] = Field(
        default_factory=lambda: {"tags": [], "run_id": None},
        description="Request metadata including tags and run_id for tracing. "
        "MUST be populated by Router, not by business code.",
    )

    # Fixed headers
    extra_headers: Dict[str, str] = Field(
        default_factory=lambda: {"x-litellm-enable-message-redaction": "true"},
        description="Governance headers (message redaction enabled by default). "
        "Uses x-litellm-enable-message-redaction per LiteLLM Proxy docs.",
    )

    # Optional BYOK config
    extra_body: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional user_config for BYOK (contains decrypted upstream API keys)",
    )

    @field_validator("metadata")
    @classmethod
    def validate_metadata(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure metadata has tags list and run_id."""
        if "tags" not in v:
            v["tags"] = []
        if not isinstance(v["tags"], list):
            raise ValueError("metadata.tags must be a list")
        if "run_id" not in v:
            v["run_id"] = None
        return v

    @field_validator("extra_headers")
    @classmethod
    def validate_extra_headers(cls, v: Dict[str, str]) -> Dict[str, str]:
        """Ensure redaction header is present."""
        if "x-litellm-enable-message-redaction" not in v:
            v["x-litellm-enable-message-redaction"] = "true"
        return v

    def add_standard_tags(
        self, scope: str, user_id: int, product: str, plan: str
    ) -> None:
        """
        Add standard tags for tracing (called by Router ONLY).

        Standard tag format:
        - scope:{scope}
        - user:{user_id}
        - product:{product}
        - plan:{plan}
        """
        if "tags" not in self.metadata:
            self.metadata["tags"] = []

        standard_tags = [
            f"scope:{scope}",
            f"user:{user_id}",
            f"product:{product}",
            f"plan:{plan}",
        ]

        for tag in standard_tags:
            if tag not in self.metadata["tags"]:
                self.metadata["tags"].append(tag)

    def set_run_id(self, run_id: str) -> None:
        """Set run_id for tracing (called by Router ONLY)."""
        self.metadata["run_id"] = run_id

    class Config:
        json_schema_extra = {
            "example": {
                "base_url": "http://litellm:4000/v1",
                "api_key": "sk-litellm-vk_system_user_123",
                "model": "sys_copilot_v1",
                "metadata": {
                    "tags": ["product:copilot", "scope:copilot", "user:123", "plan:pro"],
                    "run_id": "run_abc123xyz",
                },
                "extra_headers": {"x-litellm-enable-message-redaction": "true"},
                "extra_body": None,
            }
        }


# ============================================================================
# Database Model Schemas (for API responses)
# ============================================================================


class LLMSystemProfileResponse(BaseModel):
    """System profile response."""

    id: int
    scope: str
    proxy_model_name: str
    enabled: bool
    updated_at: datetime
    updated_by: Optional[str] = None

    class Config:
        from_attributes = True


class ByokTestCode(str, Enum):
    """Test result codes for BYOK profile validation."""
    OK = "OK"
    INVALID_KEY = "INVALID_KEY"
    RATE_LIMITED = "RATE_LIMITED"
    MODEL_NOT_FOUND = "MODEL_NOT_FOUND"
    PROVIDER_ERROR = "PROVIDER_ERROR"
    NETWORK_ERROR = "NETWORK_ERROR"


class LLMUserByokProfileResponse(BaseModel):
    """User BYOK profile response (API key NEVER included)."""

    id: int
    tier: str
    provider: str
    model: str
    api_base: Optional[str] = None
    api_version: Optional[str] = None
    enabled: bool
    key_masked: str = ""  # e.g., "sk-••••abcd"
    last_tested_at: Optional[datetime] = None
    last_test_status: Optional[str] = None  # 'pass' or 'fail'
    last_test_code: Optional[str] = None  # ByokTestCode value
    last_test_message: Optional[str] = None
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class LLMUserByokProfileCreate(BaseModel):
    """Create/update BYOK profile (api_key required)."""

    tier: LLMTier
    provider: str = Field(..., min_length=1, max_length=50)
    model: str = Field(..., min_length=1, max_length=100)
    api_key: str = Field(..., min_length=1)  # Will be encrypted server-side
    api_base: Optional[str] = None
    api_version: Optional[str] = None
    enabled: bool = True


class LLMUserByokProfileUpdate(BaseModel):
    """Update BYOK profile (api_key optional)."""

    provider: str = Field(..., min_length=1, max_length=50)
    model: str = Field(..., min_length=1, max_length=100)
    enabled: bool = True
    api_key: Optional[str] = None  # omit=keep, null=clear (but we treat null as omit for safety)


class LLMRoutingOverrideResponse(BaseModel):
    """Routing override response."""

    id: int
    scope: str
    mode: RoutingMode
    created_at: datetime
    updated_at: datetime
    updated_by: Optional[str] = None

    class Config:
        from_attributes = True


class LLMRoutingOverrideCreate(BaseModel):
    """Create/update routing override."""

    scope: LLMScope
    mode: RoutingMode


class LLMVirtualKeyResponse(BaseModel):
    """Virtual key response (key NEVER included)."""

    id: int
    key_type: VirtualKeyType
    status: VirtualKeyStatus
    allowed_models: List[str]
    created_at: datetime
    rotated_at: Optional[datetime] = None

    class Config:
        from_attributes = True


# ============================================================================
# User Context (for router input)
# ============================================================================


class UserContext(BaseModel):
    """
    User context for LLM policy routing.

    Contains all user-related information needed by the router to make
    routing decisions (BYOK eligibility, subscription status, etc.).
    """

    user_id: int
    email: Optional[str] = None
    subscription_level: str = "free"  # retained for logging/tags only
    is_active: bool = True
    byok_allowed: bool = False  # explicit permission from entitlements

    def is_byok_eligible(self) -> bool:
        """Deprecated: use byok_allowed from entitlements instead."""
        return self.byok_allowed


# ============================================================================
# Exceptions
# ============================================================================


class LLMKeyProvisioningError(Exception):
    """Raised when virtual key provisioning is in progress or failed."""

    def __init__(self, message: str, retry_after: int = 5):
        self.message = message
        self.retry_after = retry_after
        super().__init__(self.message)


class LLMByokProfileNotFoundError(Exception):
    """Raised when BYOK profile is required but not found."""

    pass


class LLMRoutingError(Exception):
    """Base exception for routing errors."""

    pass


# ============================================================================
# LLM API Schemas (from endpoints)
# ============================================================================


class ProviderSummary(BaseModel):
    """Summary of a validated provider for dropdown selection.
    
    Used in agent model configuration UI to show available providers
    that the user has configured and validated.
    """
    config_id: int
    provider_key: str
    provider_name: str
    is_validated: bool
    model_count: int
    endpoints: Optional[List[str]] = None  # For Volcano Engine


class ModelSummary(BaseModel):
    """Summary of an available model for dropdown selection.
    
    Used to populate model dropdown when user selects a provider
    in the agent model configuration UI.
    """
    model_config_id: int
    model_key: str
    model_name: str
    context_length: Optional[int] = None
    volcengine_endpoint_id: Optional[str] = None


class AgentModelConfig(BaseModel):
    """Configuration for a single agent scenario.
    
    Represents the LLM configuration for one of the agent tiers:
    - agents_fast: Quick tasks, lower cost
    - agents_balanced: Balance between speed and quality
    - agents_best: Premium quality analysis
    """
    scenario: str
    scenario_name: str
    scenario_description: str
    scenario_icon: str
    provider_config_id: Optional[int] = None
    provider_name: Optional[str] = None
    model_config_id: Optional[int] = None
    model_name: Optional[str] = None
    volcengine_endpoint: Optional[str] = None
    enabled: bool = False
    last_tested_at: Optional[datetime] = None
    last_test_status: Optional[str] = None
    last_test_message: Optional[str] = None


class AgentModelsResponse(BaseModel):
    """Response containing all agent model configurations.
    
    Returns both the current scenario configurations and
    available providers for dropdown selection.
    """
    use_own_llm_keys: bool = Field(
        default=False,
        description="Global BYOK toggle - whether user wants to use their own LLM API keys"
    )
    scenarios: List[AgentModelConfig]
    available_providers: List[ProviderSummary]


class ToggleByokRequest(BaseModel):
    """Request to toggle BYOK mode."""
    enabled: bool = Field(
        description="Whether to enable BYOK mode (use own API keys)"
    )


class ToggleByokResponse(BaseModel):
    """Response for BYOK toggle operation."""
    success: bool
    use_own_llm_keys: bool
    message: str


class UpdateAgentModelRequest(BaseModel):
    """Request to update an agent model configuration.
    
    Used when user selects a provider/model combination
    for a specific agent scenario.
    """
    provider_config_id: int
    model_config_id: int
    volcengine_endpoint: Optional[str] = None
    enabled: bool = True


class TestResultResponse(BaseModel):
    """Response from testing an agent model configuration.
    
    Contains the result of a minimal LLM call to verify
    the configuration works correctly.
    """
    scenario: str
    success: bool
    message: str
    latency_ms: Optional[int] = None


class ProviderCatalogItem(BaseModel):
    """Provider catalog item for BYOK UI.
    
    Provides minimal provider information needed for configuration:
    - provider_key: Internal identifier (e.g., "openai", "anthropic")
    - display_name: Human-readable name
    - provider_type: crewai_native, openai_compatible, etc.
    - requires_api_key: Whether API key is needed
    - requires_base_url: Whether custom base URL is required
    - default_api_base: Default API endpoint (null if not set)
    """
    provider_key: str
    display_name: str
    provider_type: str
    requires_api_key: bool
    requires_base_url: bool
    default_api_base: Optional[str] = None


class RouterStatusResponse(BaseModel):
    """Response for LLM Policy Router status check.
    
    Indicates whether the router is enabled and active.
    """
    enabled: bool
    message: str


class RoutingPreviewResponse(BaseModel):
    """Preview of what routing would be used for a scope.
    
    Shows the routing decision without making an actual LLM call:
    - routing_effective: "system" or "byok"
    - scope: The scope being queried
    - model_alias: Model alias for system routing
    - provider: Provider name (only for BYOK)
    - user_model: User's model selection (only for BYOK)
    """
    routing_effective: str  # "system" or "byok"
    scope: str
    model_alias: Optional[str] = None
    provider: Optional[str] = None  # Only for BYOK
    user_model: Optional[str] = None  # Only for BYOK


class SystemConfigReloadResponse(BaseModel):
    """Response for system config reload.
    
    Returned after hot-reloading system LLM configuration
    from environment variables.
    """
    status: str
    message: str
    scopes_available: List[str]
